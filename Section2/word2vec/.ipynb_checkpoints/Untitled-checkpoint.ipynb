{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 20:40:06,942 : INFO :loading projection weights from GoogleNews-vectors-negative300.bin\n",
      "2019-05-03 20:40:06,946 : WARNING :this function is deprecated, use smart_open.open instead\n",
      "2019-05-03 20:43:48,457 : INFO :loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "gmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
       "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
       "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ,\n",
       "        0.21484375, -0.14746094,  0.22460938, -0.125     , -0.09716797,\n",
       "        0.24902344, -0.2890625 ,  0.36523438,  0.41210938, -0.0859375 ,\n",
       "       -0.07861328, -0.19726562, -0.09082031, -0.14160156, -0.10253906,\n",
       "        0.13085938, -0.00346375,  0.07226562,  0.04418945,  0.34570312,\n",
       "        0.07470703, -0.11230469,  0.06738281,  0.11230469,  0.01977539,\n",
       "       -0.12353516,  0.20996094, -0.07226562, -0.02783203,  0.05541992,\n",
       "       -0.33398438,  0.08544922,  0.34375   ,  0.13964844,  0.04931641,\n",
       "       -0.13476562,  0.16308594, -0.37304688,  0.39648438,  0.10693359,\n",
       "        0.22167969,  0.21289062, -0.08984375,  0.20703125,  0.08935547,\n",
       "       -0.08251953,  0.05957031,  0.10205078, -0.19238281, -0.09082031,\n",
       "        0.4921875 ,  0.03955078, -0.07080078, -0.0019989 , -0.23046875,\n",
       "        0.25585938,  0.08984375, -0.10644531,  0.00105286, -0.05883789,\n",
       "        0.05102539, -0.0291748 ,  0.19335938, -0.14160156, -0.33398438,\n",
       "        0.08154297, -0.27539062,  0.10058594, -0.10449219, -0.12353516,\n",
       "       -0.140625  ,  0.03491211, -0.11767578, -0.1796875 , -0.21484375,\n",
       "       -0.23828125,  0.08447266, -0.07519531, -0.25976562, -0.21289062,\n",
       "       -0.22363281, -0.09716797,  0.11572266,  0.15429688,  0.07373047,\n",
       "       -0.27539062,  0.14257812, -0.0201416 ,  0.10009766, -0.19042969,\n",
       "       -0.09375   ,  0.14160156,  0.17089844,  0.3125    , -0.16699219,\n",
       "       -0.08691406, -0.05004883, -0.24902344, -0.20800781, -0.09423828,\n",
       "       -0.12255859, -0.09472656, -0.390625  , -0.06640625, -0.31640625,\n",
       "        0.10986328, -0.00156403,  0.04345703,  0.15625   , -0.18945312,\n",
       "       -0.03491211,  0.03393555, -0.14453125,  0.01611328, -0.14160156,\n",
       "       -0.02392578,  0.01501465,  0.07568359,  0.10742188,  0.12695312,\n",
       "        0.10693359, -0.01184082, -0.24023438,  0.0291748 ,  0.16210938,\n",
       "        0.19921875, -0.28125   ,  0.16699219, -0.11621094, -0.25585938,\n",
       "        0.38671875, -0.06640625, -0.4609375 , -0.06176758, -0.14453125,\n",
       "       -0.11621094,  0.05688477,  0.03588867, -0.10693359,  0.18847656,\n",
       "       -0.16699219, -0.01794434,  0.10986328, -0.12353516, -0.16308594,\n",
       "       -0.14453125,  0.12890625,  0.11523438,  0.13671875,  0.05688477,\n",
       "       -0.08105469, -0.06152344, -0.06689453,  0.27929688, -0.19628906,\n",
       "        0.07226562,  0.12304688, -0.20996094, -0.22070312,  0.21386719,\n",
       "       -0.1484375 , -0.05932617,  0.05224609,  0.06445312, -0.02636719,\n",
       "        0.13183594,  0.19433594,  0.27148438,  0.18652344,  0.140625  ,\n",
       "        0.06542969, -0.14453125,  0.05029297,  0.08837891,  0.12255859,\n",
       "        0.26757812,  0.0534668 , -0.32226562, -0.20703125,  0.18164062,\n",
       "        0.04418945, -0.22167969, -0.13769531, -0.04174805, -0.00286865,\n",
       "        0.04077148,  0.07275391, -0.08300781,  0.08398438, -0.3359375 ,\n",
       "       -0.40039062,  0.01757812, -0.18652344, -0.0480957 , -0.19140625,\n",
       "        0.10107422,  0.09277344, -0.30664062, -0.19921875, -0.0168457 ,\n",
       "        0.12207031,  0.14648438, -0.12890625, -0.23535156, -0.05371094,\n",
       "       -0.06640625,  0.06884766, -0.03637695,  0.2109375 , -0.06005859,\n",
       "        0.19335938,  0.05151367, -0.05322266,  0.02893066, -0.27539062,\n",
       "        0.08447266,  0.328125  ,  0.01818848,  0.01495361,  0.04711914,\n",
       "        0.37695312, -0.21875   , -0.03393555,  0.01116943,  0.36914062,\n",
       "        0.02160645,  0.03466797,  0.07275391,  0.16015625, -0.16503906,\n",
       "       -0.296875  ,  0.15039062, -0.29101562,  0.13964844,  0.00448608,\n",
       "        0.171875  , -0.21972656,  0.09326172, -0.19042969,  0.01599121,\n",
       "       -0.09228516,  0.15722656, -0.14160156, -0.0534668 ,  0.03613281,\n",
       "        0.23632812, -0.15136719, -0.00689697, -0.27148438, -0.07128906,\n",
       "       -0.16503906,  0.18457031, -0.08398438,  0.18554688,  0.11669922,\n",
       "        0.02758789, -0.04760742,  0.17871094,  0.06542969, -0.03540039,\n",
       "        0.22949219,  0.02697754, -0.09765625,  0.26953125,  0.08349609,\n",
       "       -0.13085938, -0.10107422, -0.00738525,  0.07128906,  0.14941406,\n",
       "       -0.20605469,  0.18066406, -0.15820312,  0.05932617,  0.28710938,\n",
       "       -0.04663086,  0.15136719,  0.4921875 , -0.27539062,  0.05615234],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01,\n",
       "       -8.44726562e-02,  5.73730469e-02,  5.85937500e-02, -8.25195312e-02,\n",
       "       -1.53808594e-02, -6.34765625e-02,  1.79687500e-01, -4.23828125e-01,\n",
       "       -2.25830078e-02, -1.66015625e-01, -2.51464844e-02,  1.07421875e-01,\n",
       "       -1.99218750e-01,  1.59179688e-01, -1.87500000e-01, -1.20117188e-01,\n",
       "        1.55273438e-01, -9.91210938e-02,  1.42578125e-01, -1.64062500e-01,\n",
       "       -8.93554688e-02,  2.00195312e-01, -1.49414062e-01,  3.20312500e-01,\n",
       "        3.28125000e-01,  2.44140625e-02, -9.71679688e-02, -8.20312500e-02,\n",
       "       -3.63769531e-02, -8.59375000e-02, -9.86328125e-02,  7.78198242e-03,\n",
       "       -1.34277344e-02,  5.27343750e-02,  1.48437500e-01,  3.33984375e-01,\n",
       "        1.66015625e-02, -2.12890625e-01, -1.50756836e-02,  5.24902344e-02,\n",
       "       -1.07421875e-01, -8.88671875e-02,  2.49023438e-01, -7.03125000e-02,\n",
       "       -1.59912109e-02,  7.56835938e-02, -7.03125000e-02,  1.19140625e-01,\n",
       "        2.29492188e-01,  1.41601562e-02,  1.15234375e-01,  7.50732422e-03,\n",
       "        2.75390625e-01, -2.44140625e-01,  2.96875000e-01,  3.49121094e-02,\n",
       "        2.42187500e-01,  1.35742188e-01,  1.42578125e-01,  1.75781250e-02,\n",
       "        2.92968750e-02, -1.21582031e-01,  2.28271484e-02, -4.76074219e-02,\n",
       "       -1.55273438e-01,  3.14331055e-03,  3.45703125e-01,  1.22558594e-01,\n",
       "       -1.95312500e-01,  8.10546875e-02, -6.83593750e-02, -1.47094727e-02,\n",
       "        2.14843750e-01, -1.21093750e-01,  1.57226562e-01, -2.07031250e-01,\n",
       "        1.36718750e-01, -1.29882812e-01,  5.29785156e-02, -2.71484375e-01,\n",
       "       -2.98828125e-01, -1.84570312e-01, -2.29492188e-01,  1.19140625e-01,\n",
       "        1.53198242e-02, -2.61718750e-01, -1.23046875e-01, -1.86767578e-02,\n",
       "       -6.49414062e-02, -8.15429688e-02,  7.86132812e-02, -3.53515625e-01,\n",
       "        5.24902344e-02, -2.45361328e-02, -5.43212891e-03, -2.08984375e-01,\n",
       "       -2.10937500e-01, -1.79687500e-01,  2.42187500e-01,  2.57812500e-01,\n",
       "        1.37695312e-01, -2.10937500e-01, -2.17285156e-02, -1.38671875e-01,\n",
       "        1.84326172e-02, -1.23901367e-02, -1.59179688e-01,  1.61132812e-01,\n",
       "        2.08007812e-01,  1.03027344e-01,  9.81445312e-02, -6.83593750e-02,\n",
       "       -8.72802734e-03, -2.89062500e-01, -2.14843750e-01, -1.14257812e-01,\n",
       "       -2.21679688e-01,  4.12597656e-02, -3.12500000e-01, -5.59082031e-02,\n",
       "       -9.76562500e-02,  5.81054688e-02, -4.05273438e-02, -1.73828125e-01,\n",
       "        1.64062500e-01, -2.53906250e-01, -1.54296875e-01, -2.31933594e-02,\n",
       "       -2.38281250e-01,  2.07519531e-02, -2.73437500e-01,  3.90625000e-03,\n",
       "        1.13769531e-01, -1.73828125e-01,  2.57812500e-01,  2.35351562e-01,\n",
       "        5.22460938e-02,  6.83593750e-02, -1.75781250e-01,  1.60156250e-01,\n",
       "       -5.98907471e-04,  5.98144531e-02, -2.11914062e-01, -5.54199219e-02,\n",
       "       -7.51953125e-02, -3.06640625e-01,  4.27734375e-01,  5.32226562e-02,\n",
       "       -2.08984375e-01, -5.71289062e-02, -2.09960938e-01,  3.29589844e-02,\n",
       "        1.05468750e-01, -1.50390625e-01, -9.37500000e-02,  1.16699219e-01,\n",
       "        6.44531250e-02,  2.80761719e-02,  2.41210938e-01, -1.25976562e-01,\n",
       "       -1.00585938e-01, -1.22680664e-02, -3.26156616e-04,  1.58691406e-02,\n",
       "        1.27929688e-01, -3.32031250e-02,  4.07714844e-02, -1.31835938e-01,\n",
       "        9.81445312e-02,  1.74804688e-01, -2.36328125e-01,  5.17578125e-02,\n",
       "        1.83593750e-01,  2.42919922e-02, -4.31640625e-01,  2.46093750e-01,\n",
       "       -3.03955078e-02, -2.47802734e-02, -1.17187500e-01,  1.61132812e-01,\n",
       "       -5.71289062e-02,  1.16577148e-02,  2.81250000e-01,  4.27734375e-01,\n",
       "        4.56542969e-02,  1.01074219e-01, -3.95507812e-02,  1.77001953e-02,\n",
       "       -8.98437500e-02,  1.35742188e-01,  2.08007812e-01,  1.88476562e-01,\n",
       "       -1.52343750e-01, -2.37304688e-01, -1.90429688e-01,  7.12890625e-02,\n",
       "       -2.46093750e-01, -2.61718750e-01, -2.34375000e-01, -1.45507812e-01,\n",
       "       -1.17187500e-02, -1.50390625e-01, -1.13281250e-01,  1.82617188e-01,\n",
       "        2.63671875e-01, -1.37695312e-01, -4.58984375e-01, -4.68750000e-02,\n",
       "       -1.26953125e-01, -4.22363281e-02, -1.66992188e-01,  1.26953125e-01,\n",
       "        2.59765625e-01, -2.44140625e-01, -2.19726562e-01, -8.69140625e-02,\n",
       "        1.59179688e-01, -3.78417969e-02,  8.97216797e-03, -2.77343750e-01,\n",
       "       -1.04980469e-01, -1.75781250e-01,  2.28515625e-01, -2.70996094e-02,\n",
       "        2.85156250e-01, -2.73437500e-01,  1.61132812e-02,  5.90820312e-02,\n",
       "       -2.39257812e-01,  1.77734375e-01, -1.34765625e-01,  1.38671875e-01,\n",
       "        3.53515625e-01,  1.22070312e-01,  1.43554688e-01,  9.22851562e-02,\n",
       "        2.29492188e-01, -3.00781250e-01, -4.88281250e-02, -1.79687500e-01,\n",
       "        2.96875000e-01,  1.75781250e-01,  4.80957031e-02, -3.38745117e-03,\n",
       "        7.91015625e-02, -2.38281250e-01, -2.31445312e-01,  1.66015625e-01,\n",
       "       -2.13867188e-01, -7.03125000e-02, -7.56835938e-02,  1.96289062e-01,\n",
       "       -1.29882812e-01, -1.05957031e-01, -3.53515625e-01, -1.16699219e-01,\n",
       "       -5.10253906e-02,  3.39355469e-02, -1.43554688e-01, -3.90625000e-03,\n",
       "        1.73828125e-01, -9.96093750e-02, -1.66015625e-01, -8.54492188e-02,\n",
       "       -3.82812500e-01,  5.90820312e-02, -6.22558594e-02,  8.83789062e-02,\n",
       "       -8.88671875e-02,  3.28125000e-01,  6.83593750e-02, -1.91406250e-01,\n",
       "       -8.35418701e-04,  1.04003906e-01,  1.52343750e-01, -1.53350830e-03,\n",
       "        4.16015625e-01, -3.32031250e-02,  1.49414062e-01,  2.42187500e-01,\n",
       "       -1.76757812e-01, -4.93164062e-02, -1.24511719e-01,  1.25976562e-01,\n",
       "        1.74804688e-01,  2.81250000e-01, -1.80664062e-01,  1.03027344e-01,\n",
       "       -2.75390625e-01,  2.61718750e-01,  2.46093750e-01, -4.71191406e-02,\n",
       "        6.25000000e-02,  4.16015625e-01, -3.55468750e-01,  2.22656250e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76094574"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel.similarity('cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12412613"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel.similarity('cat', 'spatula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) # strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remove apostrophres\n",
    "    sent = re.sub(r'\\W', ' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated spaces\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'review_polarity/txt_sentoken/pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4a2ba830dae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# source: http://www.cs.cornell.edu/people/pabo/movie-review-data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"review_polarity/txt_sentoken/pos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"review_polarity/txt_sentoken/neg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'review_polarity/txt_sentoken/pos'"
     ]
    }
   ],
   "source": [
    "# unsupervised training data\n",
    "import re\n",
    "import os\n",
    "unsup_sentences = []\n",
    "\n",
    "# source: http://ai.stanford.edu/~amaas/data/sentiment/, data from IMDB\n",
    "for dirname in [\"train/pos\", \"train/neg\", \"train/unsup\", \"test/pos\", \"test/neg\"]:\n",
    "    for fname in sorted(os.listdir(\"aclImdb/\"+dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(\"aclImdb/\"+dirname+\"/\"+fname, encoding='UTF-8') as f:\n",
    "                sent = f.read()\n",
    "                words = extract_words(sent)\n",
    "                unsup_sentences.append(TaggedDocument(words, [dirname+\"/\"+fname]))\n",
    "\n",
    "# source: http://www.cs.cornell.edu/people/pabo/movie-review-data\n",
    "for dirname in [\"review_polarity/txt_sentoken/pos\", \"review_polarity/txt_sentoken/neg\"]:\n",
    "    for name in sorted(os.listdir(dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(dirname+\"/\"+fname, encoding='UTF-8') as f:\n",
    "                for i, sent in enumerate(f):\n",
    "                    words = extract_words(sent)\n",
    "                    unsup_sentences.append(TaggedDocument(words, ['%s/%s-%d' % (dirname, fname, i)]))\n",
    "\n",
    "# source: https://nip.stanford.edu/sentiment/, data from Rotten Tomatoes\n",
    "with open('stanfordSentimentTreebank/original_rt_snippets.txt', encoding='UTF-8') as f:\n",
    "    for i, sent in enumerate(f):\n",
    "        words = extract_words(sent)\n",
    "        unsup_sentences.append(TaggedDocument(words, [\"rt-%d\" % i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unsup_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'hig', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'is'], tags=['train/pos/0_9.txt']),\n",
       " TaggedDocument(words=['homelessness', 'or', 'houselessness', 'as', 'george', 'carlin', 'stated', 'has', 'been', 'an', 'issue', 'for', 'years', 'but', 'never', 'a', 'plan', 'to', 'help', 'those', 'on', 'the', 'street', 'that', 'were', 'once', 'considered', 'human', 'who', 'did', 'everything', 'from', 'going', 'to', 'school', 'work', 'or', 'vote', 'for', 'the', 'matter', 'most', 'people', 'think', 'of', 'the', 'homeless', 'as', 'just', 'a', 'lost', 'cause', 'while', 'worrying', 'about', 'things', 'such', 'as', 'racism', 'the', 'war', 'on', 'iraq', 'pressuring', 'kids', 'to', 'succeed', 'technology', 'the', 'elections', 'inflation', 'or', 'worrying', 'if', 'the', 'l', 'be', 'next', 'to', 'end', 'up', 'on', 'the', 'streets', 'but', 'what', 'if', 'you', 'were', 'given', 'a', 'bet', 'to', 'live', 'on', 'the', 'streets', 'for', 'a', 'month', 'without', 'the', 'luxuries', 'you', 'once', 'had', 'from', 'a', 'home', 'the', 'entertainment', 'sets', 'a', 'bathroom', 'pictures', 'on', 'the', 'wall', 'a', 'computer', 'and', 'everything', 'you', 'once', 'treasure', 'to', 'see', 'what', 'i', 'like', 'to', 'be', 'homeless', 'that', 'is', 'goddard', 'bol', 'lesson', 'mel', 'brooks', 'who', 'directs', 'who', 'stars', 'as', 'bolt', 'plays', 'a', 'rich', 'man', 'who', 'has', 'everything', 'in', 'the', 'world', 'until', 'deciding', 'to', 'make', 'a', 'bet', 'with', 'a', 'sissy', 'rival', 'jeffery', 'tambor', 'to', 'see', 'if', 'he', 'can', 'live', 'in', 'the', 'streets', 'for', 'thirty', 'days', 'without', 'the', 'luxuries', 'if', 'bolt', 'succeeds', 'he', 'can', 'do', 'what', 'he', 'wants', 'with', 'a', 'future', 'project', 'of', 'making', 'more', 'buildings', 'the', 'be', 'on', 'where', 'bolt', 'is', 'thrown', 'on', 'the', 'street', 'with', 'a', 'bracelet', 'on', 'his', 'leg', 'to', 'monitor', 'his', 'every', 'move', 'where', 'he', 'ca', 'step', 'off', 'the', 'sidewalk', 'h', 'given', 'the', 'nickname', 'pepto', 'by', 'a', 'vagrant', 'after', 'i', 'written', 'on', 'his', 'forehead', 'where', 'bolt', 'meets', 'other', 'characters', 'including', 'a', 'woman', 'by', 'the', 'name', 'of', 'molly', 'lesley', 'ann', 'warren', 'an', 'ex', 'dancer', 'who', 'got', 'divorce', 'before', 'losing', 'her', 'home', 'and', 'her', 'pals', 'sailor', 'howard', 'morris', 'and', 'fumes', 'teddy', 'wilson', 'who', 'are', 'already', 'used', 'to', 'the', 'streets', 'the', 'e', 'survivors', 'bolt', 'is', 'h', 'not', 'used', 'to', 'reaching', 'mutual', 'agreements', 'like', 'he', 'once', 'did', 'when', 'being', 'rich', 'where', 'i', 'fight', 'or', 'flight', 'kill', 'or', 'be', 'killed', 'while', 'the', 'love', 'connection', 'between', 'molly', 'and', 'bolt', 'was', 'necessary', 'to', 'plot', 'i', 'found', 'life', 'stinks', 'to', 'be', 'one', 'of', 'mel', 'brooks', 'observant', 'films', 'where', 'prior', 'to', 'being', 'a', 'comedy', 'it', 'shows', 'a', 'tender', 'side', 'compared', 'to', 'his', 'slapstick', 'work', 'such', 'as', 'blazing', 'saddles', 'young', 'frankenstein', 'or', 'spaceballs', 'for', 'the', 'matter', 'to', 'show', 'what', 'i', 'like', 'having', 'something', 'valuable', 'before', 'losing', 'it', 'the', 'next', 'day', 'or', 'on', 'the', 'other', 'hand', 'making', 'a', 'stupid', 'bet', 'like', 'all', 'rich', 'people', 'do', 'when', 'they', 'do', 'know', 'what', 'to', 'do', 'with', 'their', 'money', 'maybe', 'they', 'should', 'give', 'it', 'to', 'the', 'homeless', 'instead', 'of', 'using', 'it', 'like', 'monopoly', 'money', 'or', 'maybe', 'this', 'film', 'will', 'inspire', 'you', 'to', 'help', 'others'], tags=['train/pos/10000_8.txt']),\n",
       " TaggedDocument(words=['brilliant', 'over', 'acting', 'by', 'lesley', 'ann', 'warren', 'best', 'dramatic', 'hobo', 'lady', 'i', 'have', 'ever', 'seen', 'and', 'love', 'scenes', 'in', 'clothes', 'warehouse', 'are', 'second', 'to', 'none', 'the', 'corn', 'on', 'face', 'is', 'a', 'classic', 'as', 'good', 'as', 'anything', 'in', 'blazing', 'saddles', 'the', 'take', 'on', 'lawyers', 'is', 'also', 'superb', 'after', 'being', 'accused', 'of', 'being', 'a', 'turncoat', 'selling', 'out', 'his', 'boss', 'and', 'being', 'dishonest', 'the', 'lawyer', 'of', 'pepto', 'bolt', 'shrugs', 'indifferently', 'a', 'lawyer', 'he', 'says', 'three', 'funny', 'words', 'jeffrey', 'tambor', 'a', 'favorite', 'from', 'the', 'later', 'larry', 'sanders', 'show', 'is', 'fantastic', 'here', 'too', 'as', 'a', 'mad', 'millionaire', 'who', 'wants', 'to', 'crush', 'the', 'ghetto', 'his', 'character', 'is', 'more', 'malevolent', 'than', 'usual', 'the', 'hospital', 'scene', 'and', 'the', 'scene', 'where', 'the', 'homeless', 'invade', 'a', 'demolition', 'site', 'are', 'all', 'time', 'classics', 'look', 'for', 'the', 'legs', 'scene', 'and', 'the', 'two', 'big', 'diggers', 'fighting', 'one', 'bleeds', 'this', 'movie', 'gets', 'better', 'each', 'time', 'i', 'see', 'it', 'which', 'is', 'quite', 'often'], tags=['train/pos/10001_10.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'easily', 'the', 'most', 'underrated', 'film', 'inn', 'the', 'brooks', 'cannon', 'sure', 'its', 'flawed', 'it', 'does', 'not', 'give', 'a', 'realistic', 'view', 'of', 'homelessness', 'unlike', 'say', 'how', 'citizen', 'kane', 'gave', 'a', 'realistic', 'view', 'of', 'lounge', 'singers', 'or', 'titanic', 'gave', 'a', 'realistic', 'view', 'of', 'italians', 'you', 'idiots', 'many', 'of', 'the', 'jokes', 'fall', 'flat', 'but', 'still', 'this', 'film', 'is', 'very', 'lovable', 'in', 'a', 'way', 'many', 'comedies', 'are', 'not', 'and', 'to', 'pull', 'that', 'off', 'in', 'a', 'story', 'about', 'some', 'of', 'the', 'most', 'traditionally', 'reviled', 'members', 'of', 'society', 'is', 'truly', 'impressive', 'its', 'not', 'the', 'fisher', 'king', 'but', 'its', 'not', 'crap', 'either', 'my', 'only', 'complaint', 'is', 'that', 'brooks', 'should', 'have', 'cast', 'someone', 'else', 'in', 'the', 'lead', 'i', 'love', 'mel', 'as', 'a', 'director', 'and', 'writer', 'not', 'so', 'much', 'as', 'a', 'lead'], tags=['train/pos/10002_7.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'not', 'the', 'typical', 'mel', 'brooks', 'film', 'it', 'was', 'much', 'less', 'slapstick', 'than', 'most', 'of', 'his', 'movies', 'and', 'actually', 'had', 'a', 'plot', 'that', 'was', 'followable', 'leslie', 'ann', 'warren', 'made', 'the', 'movie', 'she', 'is', 'such', 'a', 'fantastic', 'under', 'rated', 'actress', 'there', 'were', 'some', 'moments', 'that', 'could', 'have', 'been', 'fleshed', 'out', 'a', 'bit', 'more', 'and', 'some', 'scenes', 'that', 'could', 'probably', 'have', 'been', 'cut', 'to', 'make', 'the', 'room', 'to', 'do', 'so', 'but', 'all', 'in', 'all', 'this', 'is', 'worth', 'the', 'price', 'to', 'rent', 'and', 'see', 'it', 'the', 'acting', 'was', 'good', 'overall', 'brooks', 'himself', 'did', 'a', 'good', 'job', 'without', 'his', 'characteristic', 'speaking', 'to', 'directly', 'to', 'the', 'audience', 'again', 'warren', 'was', 'the', 'best', 'actor', 'in', 'the', 'movie', 'but', 'fume', 'and', 'sailor', 'both', 'played', 'their', 'parts', 'well'], tags=['train/pos/10003_8.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'the', 'comedic', 'robin', 'williams', 'nor', 'is', 'it', 'the', 'quirky', 'insane', 'robin', 'williams', 'of', 'recent', 'thriller', 'fame', 'this', 'is', 'a', 'hybrid', 'of', 'the', 'classic', 'drama', 'without', 'over', 'dramatization', 'mixed', 'with', 'robi', 'new', 'love', 'of', 'the', 'thriller', 'but', 'this', 'is', 'a', 'thriller', 'per', 'se', 'this', 'is', 'more', 'a', 'mystery', 'suspense', 'vehicle', 'through', 'which', 'williams', 'attempts', 'to', 'locate', 'a', 'sick', 'boy', 'and', 'his', 'keeper', 'also', 'starring', 'sandra', 'oh', 'and', 'rory', 'culkin', 'this', 'suspense', 'drama', 'plays', 'pretty', 'much', 'like', 'a', 'news', 'report', 'until', 'willia', 'character', 'gets', 'close', 'to', 'achieving', 'his', 'goal', 'i', 'must', 'say', 'that', 'i', 'was', 'highly', 'entertained', 'though', 'this', 'movie', 'fails', 'to', 'teach', 'guide', 'inspect', 'or', 'amuse', 'it', 'felt', 'more', 'like', 'i', 'was', 'watching', 'a', 'guy', 'williams', 'as', 'he', 'was', 'actually', 'performing', 'the', 'actions', 'from', 'a', 'third', 'person', 'perspective', 'in', 'other', 'words', 'it', 'felt', 'real', 'and', 'i', 'was', 'able', 'to', 'subscribe', 'to', 'the', 'premise', 'of', 'the', 'story', 'all', 'in', 'all', 'i', 'worth', 'a', 'watch', 'though', 'i', 'definitely', 'not', 'friday', 'saturday', 'night', 'fare', 'it', 'rates', 'a', '7', '7', '10', 'from', 'the', 'fiend'], tags=['train/pos/10004_8.txt']),\n",
       " TaggedDocument(words=['yes', 'its', 'an', 'art', 'to', 'successfully', 'make', 'a', 'slow', 'paced', 'thriller', 'the', 'story', 'unfolds', 'in', 'nice', 'volumes', 'while', 'you', 'do', 'even', 'notice', 'it', 'happening', 'fine', 'performance', 'by', 'robin', 'williams', 'the', 'sexuality', 'angles', 'in', 'the', 'film', 'can', 'seem', 'unnecessary', 'and', 'can', 'probably', 'affect', 'how', 'much', 'you', 'enjoy', 'the', 'film', 'however', 'the', 'core', 'plot', 'is', 'very', 'engaging', 'the', 'movie', 'does', 'rush', 'onto', 'you', 'and', 'still', 'grips', 'you', 'enough', 'to', 'keep', 'you', 'wondering', 'the', 'direction', 'is', 'good', 'use', 'of', 'lights', 'to', 'achieve', 'desired', 'affects', 'of', 'suspense', 'and', 'unexpectedness', 'is', 'good', 'very', 'nice', '1', 'time', 'watch', 'if', 'you', 'are', 'looking', 'to', 'lay', 'back', 'and', 'hear', 'a', 'thrilling', 'short', 'story'], tags=['train/pos/10005_7.txt']),\n",
       " TaggedDocument(words=['in', 'this', 'critically', 'acclaimed', 'psychological', 'thriller', 'based', 'on', 'true', 'events', 'gabriel', 'robin', 'williams', 'a', 'celebrated', 'writer', 'and', 'late', 'night', 'talk', 'show', 'host', 'becomes', 'captivated', 'by', 'the', 'harrowing', 'story', 'of', 'a', 'young', 'listener', 'and', 'his', 'adoptive', 'mother', 'toni', 'collette', 'when', 'troubling', 'questions', 'arise', 'about', 'this', 'bo', 'story', 'however', 'gabriel', 'finds', 'himself', 'drawn', 'into', 'a', 'widening', 'mystery', 'that', 'hides', 'a', 'deadly', 'secret', 'according', 'to', 'fil', 'official', 'synopsis', 'you', 'really', 'should', 'stop', 'reading', 'these', 'comments', 'and', 'watch', 'the', 'film', 'now', 'the', 'how', 'did', 'he', 'lose', 'his', 'leg', 'ending', 'with', 'ms', 'collette', 'planning', 'her', 'new', 'life', 'should', 'be', 'chopped', 'off', 'and', 'sent', 'to', 'deleted', 'scenes', 'land', 'i', 'overkill', 'the', 'true', 'nature', 'of', 'her', 'physical', 'and', 'mental', 'ailments', 'should', 'be', 'obvious', 'by', 'the', 'time', 'mr', 'williams', 'returns', 'to', 'new', 'york', 'possibly', 'her', 'blindness', 'could', 'be', 'in', 'question', 'but', 'a', 'revelation', 'could', 'have', 'be', 'made', 'certain', 'in', 'either', 'the', 'highway', 'or', 'video', 'tape', 'scenes', 'the', 'film', 'would', 'benefit', 'from', 'a', 're', 'editing', 'how', 'about', 'a', 'directo', 'cut', 'williams', 'and', 'bobby', 'cannavale', 'as', 'jess', 'do', 'seem', 'initially', 'believable', 'as', 'a', 'couple', 'a', 'scene', 'or', 'two', 'establishing', 'their', 'relationship', 'might', 'have', 'helped', 'set', 'the', 'stage', 'otherwise', 'the', 'cast', 'is', 'exemplary', 'williams', 'offers', 'an', 'exceptionally', 'strong', 'characterization', 'and', 'not', 'a', 'gay', 'impersonation', 'sandra', 'oh', 'as', 'anna', 'joe', 'morton', 'as', 'ashe', 'and', 'rory', 'culkin', 'pete', 'logand', 'are', 'all', 'perfect', 'best', 'of', 'all', 'collett', 'donna', 'belongs', 'in', 'the', 'creepy', 'hall', 'of', 'fame', 'ms', 'oh', 'is', 'correct', 'in', 'saying', 'collette', 'might', 'be', 'you', 'know', 'like', 'that', 'guy', 'from', 'psycho', 'there', 'have', 'been', 'several', 'years', 'when', 'organizations', 'giving', 'acting', 'awards', 'seemed', 'to', 'reach', 'for', 'women', 'due', 'to', 'a', 'slighter', 'dispersion', 'of', 'roles', 'certainly', 'they', 'could', 'have', 'noticed', 'collette', 'with', 'some', 'award', 'consideration', 'she', 'is', 'that', 'good', 'and', 'director', 'patrick', 'stettner', 'definitely', 'evokes', 'hitchcock', 'he', 'even', 'makes', 'getting', 'a', 'sandwich', 'from', 'a', 'vending', 'machine', 'suspenseful', 'finally', 'writers', 'stettner', 'armistead', 'maupin', 'and', 'terry', 'anderson', 'deserve', 'gratitude', 'from', 'flight', 'attendants', 'everywhere', 'the', 'night', 'listener', '1', '21', '06', 'patrick', 'stettner', 'robin', 'williams', 'toni', 'collette', 'sandra', 'oh', 'rory', 'culkin'], tags=['train/pos/10006_7.txt']),\n",
       " TaggedDocument(words=['the', 'night', 'listener', '2006', '1', '2', 'robin', 'williams', 'toni', 'collette', 'bobby', 'cannavale', 'rory', 'culkin', 'joe', 'morton', 'sandra', 'oh', 'john', 'cullum', 'lisa', 'emery', 'becky', 'ann', 'baker', 'dir', 'patrick', 'stettner', 'hitchcockian', 'suspenser', 'gives', 'williams', 'a', 'stand', 'out', 'low', 'key', 'performance', 'what', 'is', 'it', 'about', 'celebrities', 'and', 'fans', 'what', 'is', 'the', 'near', 'paranoia', 'one', 'associates', 'with', 'the', 'other', 'and', 'why', 'is', 'it', 'almost', 'the', 'norm', 'in', 'the', 'latest', 'derange', 'fan', 'scenario', 'based', 'on', 'true', 'events', 'no', 'less', 'williams', 'stars', 'as', 'a', 'talk', 'radio', 'personality', 'named', 'gabriel', 'no', 'one', 'who', 'reads', 'stories', 'h', 'penned', 'over', 'the', 'airwaves', 'and', 'has', 'accumulated', 'an', 'interesting', 'fan', 'in', 'the', 'form', 'of', 'a', 'young', 'boy', 'named', 'pete', 'logand', 'culkin', 'who', 'has', 'submitted', 'a', 'manuscript', 'about', 'the', 'travails', 'of', 'his', 'troubled', 'youth', 'to', 'no', 'on', 'editor', 'ashe', 'morton', 'who', 'gives', 'it', 'to', 'no', 'one', 'to', 'read', 'for', 'himself', 'no', 'one', 'is', 'naturally', 'disturbed', 'but', 'ultimately', 'intrigued', 'about', 'the', 'nightmarish', 'existence', 'of', 'pete', 'being', 'abducted', 'and', 'sexually', 'abused', 'for', 'years', 'until', 'he', 'was', 'finally', 'rescued', 'by', 'a', 'nurse', 'named', 'donna', 'collette', 'giving', 'an', 'excellent', 'performance', 'who', 'has', 'adopted', 'the', 'boy', 'but', 'her', 'correspondence', 'with', 'no', 'one', 'reveals', 'that', 'pete', 'is', 'dying', 'from', 'aids', 'naturally', 'no', 'one', 'wants', 'to', 'meet', 'the', 'fans', 'but', 'is', 'suddenly', 'in', 'doubt', 'to', 'their', 'possibly', 'devious', 'ulterior', 'motives', 'when', 'the', 'seed', 'is', 'planted', 'by', 'his', 'estranged', 'lover', 'jess', 'cannavale', 'whose', 'sudden', 'departure', 'from', 'their', 'new', 'york', 'city', 'apartment', 'has', 'no', 'one', 'in', 'an', 'emotional', 'tailspin', 'that', 'has', 'only', 'now', 'grown', 'into', 'a', 'tempest', 'in', 'a', 'teacup', 'when', 'he', 'decides', 'to', 'do', 'some', 'investigating', 'into', 'donna', 'and', 'pet', 'backgrounds', 'discovering', 'some', 'truths', 'that', 'he', 'did', 'anticipate', 'written', 'by', 'armistead', 'maupin', 'who', 'co', 'wrote', 'the', 'screenplay', 'with', 'his', 'former', 'lover', 'terry', 'anderson', 'and', 'the', 'fil', 'novice', 'director', 'stettner', 'and', 'based', 'on', 'a', 'true', 'story', 'about', 'a', 'fa', 'hoax', 'found', 'out', 'has', 'some', 'hitchcockian', 'moments', 'that', 'run', 'on', 'full', 'tilt', 'like', 'any', 'good', 'old', 'fashioned', 'pot', 'boiler', 'does', 'it', 'helps', 'that', 'williams', 'gives', 'a', 'stand', 'out', 'low', 'key', 'performance', 'as', 'the', 'conflicted', 'good', 'hearted', 'personality', 'who', 'genuinely', 'wants', 'to', 'believe', 'that', 'his', 'number', 'one', 'fan', 'is', 'in', 'fact', 'real', 'and', 'does', 'love', 'him', 'the', 'one', 'thing', 'that', 'has', 'escaped', 'his', 'own', 'reality', 'and', 'has', 'some', 'unsettling', 'dreadful', 'moments', 'with', 'the', 'creepy', 'collette', 'whose', 'one', 'physical', 'trait', 'i', 'will', 'leave', 'unmentioned', 'but', 'underlines', 'the', 'desperation', 'of', 'her', 'character', 'that', 'can', 'rattle', 'you', 'to', 'the', 'core', 'however', 'the', 'film', 'runs', 'out', 'of', 'gas', 'and', 'eventually', 'becomes', 'a', 'bit', 'repetitive', 'and', 'predictable', 'despite', 'a', 'finely', 'directed', 'piece', 'of', 'hoodwink', 'and', 'mystery', 'by', 'stettner', 'it', 'pays', 'to', 'listen', 'to', 'your', 'own', 'inner', 'voice', 'be', 'careful', 'of', 'what', 'you', 'hope', 'for'], tags=['train/pos/10007_7.txt']),\n",
       " TaggedDocument(words=['you', 'know', 'robin', 'williams', 'god', 'bless', 'him', 'is', 'constantly', 'shooting', 'himself', 'in', 'the', 'foot', 'lately', 'with', 'all', 'these', 'dumb', 'comedies', 'he', 'has', 'done', 'this', 'decade', 'with', 'perhaps', 'the', 'exception', 'of', 'death', 'to', 'smoochy', 'which', 'bombed', 'when', 'it', 'came', 'out', 'but', 'is', 'now', 'a', 'cult', 'classic', 'the', 'dramas', 'he', 'has', 'made', 'lately', 'have', 'been', 'fantastic', 'especially', 'insomnia', 'and', 'one', 'hour', 'photo', 'the', 'night', 'listener', 'despite', 'mediocre', 'reviews', 'and', 'a', 'quick', 'dvd', 'release', 'is', 'among', 'his', 'best', 'work', 'period', 'this', 'is', 'a', 'very', 'chilling', 'story', 'even', 'though', 'it', 'does', 'include', 'a', 'serial', 'killer', 'or', 'anyone', 'that', 'physically', 'dangerous', 'for', 'that', 'matter', 'the', 'concept', 'of', 'the', 'film', 'is', 'based', 'on', 'an', 'actual', 'case', 'of', 'fraud', 'that', 'still', 'has', 'yet', 'to', 'be', 'officially', 'confirmed', 'in', 'high', 'school', 'i', 'read', 'an', 'autobiography', 'by', 'a', 'child', 'named', 'anthony', 'godby', 'johnson', 'who', 'suffered', 'horrific', 'abuse', 'and', 'eventually', 'contracted', 'aids', 'as', 'a', 'result', 'i', 'was', 'moved', 'by', 'the', 'story', 'until', 'i', 'read', 'reports', 'online', 'that', 'johnson', 'may', 'not', 'actually', 'exist', 'when', 'i', 'saw', 'this', 'movie', 'the', 'confused', 'feelings', 'that', 'robin', 'williams', 'so', 'brilliantly', 'portrayed', 'resurfaced', 'in', 'my', 'mind', 'toni', 'collette', 'probably', 'gives', 'her', 'best', 'dramatic', 'performance', 'too', 'as', 'the', 'ultimately', 'sociopathic', 'caretaker', 'her', 'role', 'was', 'a', 'far', 'cry', 'from', 'those', 'she', 'had', 'in', 'movies', 'like', 'little', 'miss', 'sunshine', 'there', 'were', 'even', 'times', 'she', 'looked', 'into', 'the', 'camera', 'where', 'i', 'thought', 'she', 'was', 'staring', 'right', 'at', 'me', 'it', 'takes', 'a', 'good', 'actress', 'to', 'play', 'that', 'sort', 'of', 'role', 'and', 'i', 'this', 'understated', 'yet', 'well', 'reviewed', 'role', 'that', 'makes', 'toni', 'collette', 'probably', 'one', 'of', 'the', 'best', 'actresses', 'of', 'this', 'generation', 'not', 'to', 'have', 'even', 'been', 'nominated', 'for', 'an', 'academy', 'award', 'as', 'of', '2008', 'i', 'incredible', 'that', 'there', 'is', 'at', 'least', 'one', 'woman', 'in', 'this', 'world', 'who', 'is', 'like', 'this', 'and', 'i', 'scary', 'too', 'this', 'is', 'a', 'good', 'dark', 'film', 'that', 'i', 'highly', 'recommend', 'be', 'prepared', 'to', 'be', 'unsettled', 'though', 'because', 'this', 'movie', 'leaves', 'you', 'with', 'a', 'strange', 'feeling', 'at', 'the', 'end'], tags=['train/pos/10008_7.txt'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class PermuteSentences(object):\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "    \n",
    "    def __iter__(self):\n",
    "        shuffled = list(self.sents)\n",
    "        random.shuffle(shuffled)\n",
    "        for sent in shuffled:\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 21:28:38,298 : WARNING :consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-03 21:28:38,301 : INFO :collecting all words and their counts\n",
      "2019-05-03 21:28:38,477 : INFO :PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-05-03 21:28:39,457 : INFO :PROGRESS: at example #10000, processed 2329862 words (2380499/s), 55889 word types, 10000 tags\n",
      "2019-05-03 21:28:40,413 : INFO :PROGRESS: at example #20000, processed 4653233 words (2436010/s), 75510 word types, 20000 tags\n",
      "2019-05-03 21:28:41,371 : INFO :PROGRESS: at example #30000, processed 7018694 words (2471688/s), 90271 word types, 30000 tags\n",
      "2019-05-03 21:28:42,285 : INFO :PROGRESS: at example #40000, processed 9357764 words (2564333/s), 102403 word types, 40000 tags\n",
      "2019-05-03 21:28:43,212 : INFO :PROGRESS: at example #50000, processed 11661800 words (2489474/s), 112374 word types, 50000 tags\n",
      "2019-05-03 21:28:47,713 : INFO :PROGRESS: at example #60000, processed 13986144 words (516578/s), 121385 word types, 60000 tags\n",
      "2019-05-03 21:28:48,634 : INFO :PROGRESS: at example #70000, processed 16281860 words (2496313/s), 129531 word types, 70000 tags\n",
      "2019-05-03 21:28:49,497 : INFO :PROGRESS: at example #80000, processed 18629827 words (2724052/s), 137587 word types, 80000 tags\n",
      "2019-05-03 21:28:50,356 : INFO :PROGRESS: at example #90000, processed 20940101 words (2693786/s), 144557 word types, 90000 tags\n",
      "2019-05-03 21:28:51,284 : INFO :collected 151176 word types and 100000 unique tags from a corpus of 100000 examples and 23267059 words\n",
      "2019-05-03 21:28:51,286 : INFO :Loading a fresh vocabulary\n",
      "2019-05-03 21:28:51,593 : INFO :effective_min_count=5 retains 56163 unique words (37% of original 151176, drops 95013)\n",
      "2019-05-03 21:28:51,594 : INFO :effective_min_count=5 leaves 23115013 word corpus (99% of original 23267059, drops 152046)\n",
      "2019-05-03 21:28:51,969 : INFO :deleting the raw counts dictionary of 151176 items\n",
      "2019-05-03 21:28:51,977 : INFO :sample=0.001 downsamples 47 most-common words\n",
      "2019-05-03 21:28:51,978 : INFO :downsampling leaves estimated 17462348 word corpus (75.5% of prior 23115013)\n",
      "2019-05-03 21:28:52,083 : INFO :constructing a huffman tree from 56163 words\n",
      "2019-05-03 21:28:55,488 : INFO :built huffman tree with maximum node depth 22\n",
      "2019-05-03 21:28:55,700 : INFO :estimated required memory for 56163 words and 50 dimensions: 113011900 bytes\n",
      "2019-05-03 21:28:55,702 : INFO :resetting layer weights\n",
      "2019-05-03 21:28:59,985 : INFO :training model with 3 workers on 56163 vocabulary and 50 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2019-05-03 21:29:01,009 : INFO :EPOCH 1 - PROGRESS: at 1.71% examples, 297795 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:02,066 : INFO :EPOCH 1 - PROGRESS: at 3.17% examples, 271008 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:03,083 : INFO :EPOCH 1 - PROGRESS: at 4.69% examples, 265774 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:04,092 : INFO :EPOCH 1 - PROGRESS: at 6.56% examples, 279868 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:05,127 : INFO :EPOCH 1 - PROGRESS: at 8.26% examples, 282819 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:06,132 : INFO :EPOCH 1 - PROGRESS: at 10.13% examples, 291027 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:07,150 : INFO :EPOCH 1 - PROGRESS: at 11.69% examples, 288811 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:08,181 : INFO :EPOCH 1 - PROGRESS: at 13.11% examples, 283328 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:09,188 : INFO :EPOCH 1 - PROGRESS: at 14.86% examples, 285355 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:10,200 : INFO :EPOCH 1 - PROGRESS: at 16.72% examples, 288900 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:11,238 : INFO :EPOCH 1 - PROGRESS: at 18.51% examples, 290040 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:12,270 : INFO :EPOCH 1 - PROGRESS: at 20.04% examples, 288048 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:13,272 : INFO :EPOCH 1 - PROGRESS: at 21.88% examples, 290357 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:14,321 : INFO :EPOCH 1 - PROGRESS: at 23.67% examples, 291227 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:15,324 : INFO :EPOCH 1 - PROGRESS: at 25.31% examples, 291067 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:16,335 : INFO :EPOCH 1 - PROGRESS: at 27.00% examples, 290775 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:17,353 : INFO :EPOCH 1 - PROGRESS: at 28.55% examples, 289504 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:18,378 : INFO :EPOCH 1 - PROGRESS: at 30.17% examples, 289164 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:19,382 : INFO :EPOCH 1 - PROGRESS: at 31.70% examples, 287948 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:20,399 : INFO :EPOCH 1 - PROGRESS: at 33.21% examples, 286344 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:21,417 : INFO :EPOCH 1 - PROGRESS: at 34.80% examples, 286195 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:22,427 : INFO :EPOCH 1 - PROGRESS: at 36.32% examples, 285266 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:23,428 : INFO :EPOCH 1 - PROGRESS: at 37.91% examples, 285132 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:24,445 : INFO :EPOCH 1 - PROGRESS: at 39.82% examples, 287215 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:25,451 : INFO :EPOCH 1 - PROGRESS: at 41.74% examples, 288973 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:26,468 : INFO :EPOCH 1 - PROGRESS: at 43.59% examples, 289926 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:27,497 : INFO :EPOCH 1 - PROGRESS: at 45.59% examples, 291218 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:28,504 : INFO :EPOCH 1 - PROGRESS: at 47.48% examples, 292872 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:29,507 : INFO :EPOCH 1 - PROGRESS: at 49.44% examples, 294763 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:30,552 : INFO :EPOCH 1 - PROGRESS: at 51.23% examples, 295150 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:31,570 : INFO :EPOCH 1 - PROGRESS: at 53.14% examples, 296209 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:32,607 : INFO :EPOCH 1 - PROGRESS: at 55.14% examples, 297486 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:33,614 : INFO :EPOCH 1 - PROGRESS: at 57.06% examples, 298720 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:34,635 : INFO :EPOCH 1 - PROGRESS: at 59.06% examples, 300200 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:35,645 : INFO :EPOCH 1 - PROGRESS: at 60.95% examples, 300851 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:36,683 : INFO :EPOCH 1 - PROGRESS: at 63.02% examples, 302050 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:37,691 : INFO :EPOCH 1 - PROGRESS: at 65.08% examples, 303266 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:38,693 : INFO :EPOCH 1 - PROGRESS: at 66.97% examples, 304024 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:39,707 : INFO :EPOCH 1 - PROGRESS: at 68.89% examples, 304837 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:40,720 : INFO :EPOCH 1 - PROGRESS: at 70.78% examples, 305119 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:41,723 : INFO :EPOCH 1 - PROGRESS: at 72.66% examples, 305749 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 21:29:42,751 : INFO :EPOCH 1 - PROGRESS: at 74.59% examples, 306527 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:43,790 : INFO :EPOCH 1 - PROGRESS: at 76.56% examples, 307231 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:44,795 : INFO :EPOCH 1 - PROGRESS: at 78.67% examples, 308433 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:45,820 : INFO :EPOCH 1 - PROGRESS: at 80.66% examples, 309003 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 21:29:46,828 : INFO :EPOCH 1 - PROGRESS: at 82.77% examples, 310271 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:47,838 : INFO :EPOCH 1 - PROGRESS: at 84.86% examples, 311442 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:48,842 : INFO :EPOCH 1 - PROGRESS: at 87.24% examples, 313675 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:49,849 : INFO :EPOCH 1 - PROGRESS: at 89.74% examples, 316119 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:50,854 : INFO :EPOCH 1 - PROGRESS: at 92.17% examples, 318335 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 21:29:51,862 : INFO :EPOCH 1 - PROGRESS: at 94.38% examples, 319562 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:52,876 : INFO :EPOCH 1 - PROGRESS: at 96.73% examples, 321389 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:53,889 : INFO :EPOCH 1 - PROGRESS: at 98.82% examples, 322081 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 21:29:54,423 : INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 21:29:54,426 : INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 21:29:54,449 : INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 21:29:54,450 : INFO :EPOCH - 1 : training on 23267059 raw words (17562769 effective words) took 54.4s, 322593 effective words/s\n",
      "2019-05-03 21:29:55,481 : INFO :EPOCH 2 - PROGRESS: at 1.68% examples, 289949 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:56,489 : INFO :EPOCH 2 - PROGRESS: at 3.62% examples, 310783 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 21:29:57,513 : INFO :EPOCH 2 - PROGRESS: at 5.31% examples, 303755 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:58,525 : INFO :EPOCH 2 - PROGRESS: at 7.58% examples, 326633 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:29:59,542 : INFO :EPOCH 2 - PROGRESS: at 10.26% examples, 354505 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:00,545 : INFO :EPOCH 2 - PROGRESS: at 12.68% examples, 366567 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:01,555 : INFO :EPOCH 2 - PROGRESS: at 15.02% examples, 371885 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:02,578 : INFO :EPOCH 2 - PROGRESS: at 17.68% examples, 382567 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:03,579 : INFO :EPOCH 2 - PROGRESS: at 20.12% examples, 388585 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:04,586 : INFO :EPOCH 2 - PROGRESS: at 22.67% examples, 393366 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:05,618 : INFO :EPOCH 2 - PROGRESS: at 24.96% examples, 393539 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:06,674 : INFO :EPOCH 2 - PROGRESS: at 26.92% examples, 387512 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:07,691 : INFO :EPOCH 2 - PROGRESS: at 29.14% examples, 386334 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:08,746 : INFO :EPOCH 2 - PROGRESS: at 31.78% examples, 390362 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:09,747 : INFO :EPOCH 2 - PROGRESS: at 34.04% examples, 391053 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:10,748 : INFO :EPOCH 2 - PROGRESS: at 36.36% examples, 391981 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:11,750 : INFO :EPOCH 2 - PROGRESS: at 38.61% examples, 392045 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:12,760 : INFO :EPOCH 2 - PROGRESS: at 41.05% examples, 394097 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 21:30:13,792 : INFO :EPOCH 2 - PROGRESS: at 43.38% examples, 394459 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:14,833 : INFO :EPOCH 2 - PROGRESS: at 44.72% examples, 385633 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:15,841 : INFO :EPOCH 2 - PROGRESS: at 46.58% examples, 383036 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:16,876 : INFO :EPOCH 2 - PROGRESS: at 49.05% examples, 385216 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:17,911 : INFO :EPOCH 2 - PROGRESS: at 50.86% examples, 381840 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:18,930 : INFO :EPOCH 2 - PROGRESS: at 53.12% examples, 382028 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:19,961 : INFO :EPOCH 2 - PROGRESS: at 55.45% examples, 382546 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:20,962 : INFO :EPOCH 2 - PROGRESS: at 57.85% examples, 384027 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:21,966 : INFO :EPOCH 2 - PROGRESS: at 59.75% examples, 382418 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:23,007 : INFO :EPOCH 2 - PROGRESS: at 61.65% examples, 379875 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:24,026 : INFO :EPOCH 2 - PROGRESS: at 64.33% examples, 382832 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:25,041 : INFO :EPOCH 2 - PROGRESS: at 66.89% examples, 384860 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:26,064 : INFO :EPOCH 2 - PROGRESS: at 68.73% examples, 382525 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:27,093 : INFO :EPOCH 2 - PROGRESS: at 71.32% examples, 384704 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:28,108 : INFO :EPOCH 2 - PROGRESS: at 73.88% examples, 386302 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:29,131 : INFO :EPOCH 2 - PROGRESS: at 76.49% examples, 388084 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:30,131 : INFO :EPOCH 2 - PROGRESS: at 79.19% examples, 390272 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:31,143 : INFO :EPOCH 2 - PROGRESS: at 81.79% examples, 391859 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:32,149 : INFO :EPOCH 2 - PROGRESS: at 84.43% examples, 393417 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:33,179 : INFO :EPOCH 2 - PROGRESS: at 87.08% examples, 395002 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:34,187 : INFO :EPOCH 2 - PROGRESS: at 89.38% examples, 394922 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:35,189 : INFO :EPOCH 2 - PROGRESS: at 92.06% examples, 396651 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:36,197 : INFO :EPOCH 2 - PROGRESS: at 94.48% examples, 397398 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:37,198 : INFO :EPOCH 2 - PROGRESS: at 96.96% examples, 398132 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:38,226 : INFO :EPOCH 2 - PROGRESS: at 99.53% examples, 399300 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:38,415 : INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 21:30:38,417 : INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 21:30:38,449 : INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 21:30:38,450 : INFO :EPOCH - 2 : training on 23267059 raw words (17560214 effective words) took 44.0s, 399178 effective words/s\n",
      "2019-05-03 21:30:39,466 : INFO :EPOCH 3 - PROGRESS: at 2.09% examples, 361928 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:40,488 : INFO :EPOCH 3 - PROGRESS: at 4.26% examples, 369059 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:41,496 : INFO :EPOCH 3 - PROGRESS: at 6.20% examples, 356200 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:42,506 : INFO :EPOCH 3 - PROGRESS: at 8.36% examples, 360952 words/s, in_qsize 5, out_qsize 1\n",
      "2019-05-03 21:30:43,528 : INFO :EPOCH 3 - PROGRESS: at 10.17% examples, 351059 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:44,530 : INFO :EPOCH 3 - PROGRESS: at 11.57% examples, 333524 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:45,554 : INFO :EPOCH 3 - PROGRESS: at 13.22% examples, 326055 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:46,565 : INFO :EPOCH 3 - PROGRESS: at 15.15% examples, 326796 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:47,601 : INFO :EPOCH 3 - PROGRESS: at 17.36% examples, 332861 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:48,620 : INFO :EPOCH 3 - PROGRESS: at 19.80% examples, 341111 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:49,641 : INFO :EPOCH 3 - PROGRESS: at 22.24% examples, 348342 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:50,645 : INFO :EPOCH 3 - PROGRESS: at 24.86% examples, 357449 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:51,645 : INFO :EPOCH 3 - PROGRESS: at 27.64% examples, 366867 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:52,653 : INFO :EPOCH 3 - PROGRESS: at 30.05% examples, 370729 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:53,661 : INFO :EPOCH 3 - PROGRESS: at 32.60% examples, 376324 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:54,661 : INFO :EPOCH 3 - PROGRESS: at 34.93% examples, 379038 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:55,672 : INFO :EPOCH 3 - PROGRESS: at 37.49% examples, 383133 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:56,672 : INFO :EPOCH 3 - PROGRESS: at 39.86% examples, 384463 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:57,687 : INFO :EPOCH 3 - PROGRESS: at 42.43% examples, 386893 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:30:58,692 : INFO :EPOCH 3 - PROGRESS: at 44.87% examples, 388970 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 21:30:59,695 : INFO :EPOCH 3 - PROGRESS: at 47.13% examples, 389104 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:00,696 : INFO :EPOCH 3 - PROGRESS: at 49.40% examples, 389879 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:01,699 : INFO :EPOCH 3 - PROGRESS: at 51.63% examples, 390048 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:02,708 : INFO :EPOCH 3 - PROGRESS: at 54.12% examples, 392122 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:03,718 : INFO :EPOCH 3 - PROGRESS: at 56.53% examples, 392893 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:04,718 : INFO :EPOCH 3 - PROGRESS: at 58.83% examples, 393148 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:05,729 : INFO :EPOCH 3 - PROGRESS: at 61.34% examples, 394902 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:06,750 : INFO :EPOCH 3 - PROGRESS: at 63.69% examples, 395552 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:07,776 : INFO :EPOCH 3 - PROGRESS: at 66.18% examples, 396896 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 21:31:08,788 : INFO :EPOCH 3 - PROGRESS: at 68.42% examples, 396857 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:09,798 : INFO :EPOCH 3 - PROGRESS: at 70.57% examples, 396174 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:10,838 : INFO :EPOCH 3 - PROGRESS: at 73.02% examples, 396673 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:11,858 : INFO :EPOCH 3 - PROGRESS: at 75.54% examples, 397260 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:12,862 : INFO :EPOCH 3 - PROGRESS: at 77.83% examples, 397308 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:13,864 : INFO :EPOCH 3 - PROGRESS: at 80.02% examples, 396893 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:14,885 : INFO :EPOCH 3 - PROGRESS: at 82.44% examples, 397188 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 21:31:15,896 : INFO :EPOCH 3 - PROGRESS: at 84.96% examples, 398389 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:16,900 : INFO :EPOCH 3 - PROGRESS: at 87.06% examples, 397801 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:17,922 : INFO :EPOCH 3 - PROGRESS: at 89.07% examples, 396672 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:18,938 : INFO :EPOCH 3 - PROGRESS: at 90.74% examples, 393707 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:19,955 : INFO :EPOCH 3 - PROGRESS: at 92.32% examples, 390831 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:21,005 : INFO :EPOCH 3 - PROGRESS: at 93.83% examples, 387639 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:22,025 : INFO :EPOCH 3 - PROGRESS: at 95.93% examples, 387096 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:23,041 : INFO :EPOCH 3 - PROGRESS: at 98.35% examples, 387581 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:23,664 : INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 21:31:23,673 : INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 21:31:23,676 : INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 21:31:23,678 : INFO :EPOCH - 3 : training on 23267059 raw words (17563434 effective words) took 45.2s, 388412 effective words/s\n",
      "2019-05-03 21:31:24,711 : INFO :EPOCH 4 - PROGRESS: at 1.74% examples, 297659 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:25,720 : INFO :EPOCH 4 - PROGRESS: at 3.75% examples, 322053 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:26,727 : INFO :EPOCH 4 - PROGRESS: at 5.62% examples, 323079 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:27,731 : INFO :EPOCH 4 - PROGRESS: at 7.82% examples, 337827 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:28,741 : INFO :EPOCH 4 - PROGRESS: at 9.86% examples, 342250 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:29,747 : INFO :EPOCH 4 - PROGRESS: at 12.31% examples, 356643 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:30,757 : INFO :EPOCH 4 - PROGRESS: at 14.36% examples, 356121 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:31,779 : INFO :EPOCH 4 - PROGRESS: at 16.52% examples, 358840 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:32,797 : INFO :EPOCH 4 - PROGRESS: at 18.47% examples, 356891 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:33,815 : INFO :EPOCH 4 - PROGRESS: at 20.44% examples, 355293 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:34,838 : INFO :EPOCH 4 - PROGRESS: at 22.76% examples, 359404 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:35,853 : INFO :EPOCH 4 - PROGRESS: at 25.22% examples, 365923 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:36,875 : INFO :EPOCH 4 - PROGRESS: at 27.73% examples, 370857 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:37,885 : INFO :EPOCH 4 - PROGRESS: at 30.16% examples, 375190 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:38,890 : INFO :EPOCH 4 - PROGRESS: at 32.54% examples, 377235 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:39,897 : INFO :EPOCH 4 - PROGRESS: at 34.37% examples, 373034 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:40,904 : INFO :EPOCH 4 - PROGRESS: at 36.25% examples, 370263 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:41,958 : INFO :EPOCH 4 - PROGRESS: at 38.36% examples, 369169 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:42,974 : INFO :EPOCH 4 - PROGRESS: at 40.74% examples, 371680 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:44,010 : INFO :EPOCH 4 - PROGRESS: at 42.69% examples, 369447 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:45,027 : INFO :EPOCH 4 - PROGRESS: at 44.53% examples, 367459 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:46,041 : INFO :EPOCH 4 - PROGRESS: at 46.56% examples, 367296 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:47,047 : INFO :EPOCH 4 - PROGRESS: at 48.89% examples, 368634 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:48,062 : INFO :EPOCH 4 - PROGRESS: at 51.22% examples, 370307 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:49,071 : INFO :EPOCH 4 - PROGRESS: at 53.66% examples, 372497 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:50,090 : INFO :EPOCH 4 - PROGRESS: at 56.12% examples, 374174 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:51,113 : INFO :EPOCH 4 - PROGRESS: at 58.21% examples, 373132 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:52,134 : INFO :EPOCH 4 - PROGRESS: at 60.09% examples, 371440 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:53,153 : INFO :EPOCH 4 - PROGRESS: at 62.20% examples, 370942 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:54,172 : INFO :EPOCH 4 - PROGRESS: at 64.47% examples, 371852 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:55,187 : INFO :EPOCH 4 - PROGRESS: at 66.90% examples, 373335 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:56,209 : INFO :EPOCH 4 - PROGRESS: at 68.91% examples, 372270 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:57,232 : INFO :EPOCH 4 - PROGRESS: at 70.96% examples, 371763 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:58,242 : INFO :EPOCH 4 - PROGRESS: at 73.08% examples, 371626 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:31:59,250 : INFO :EPOCH 4 - PROGRESS: at 75.61% examples, 373592 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:00,271 : INFO :EPOCH 4 - PROGRESS: at 78.18% examples, 375556 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:01,283 : INFO :EPOCH 4 - PROGRESS: at 80.45% examples, 375878 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:02,284 : INFO :EPOCH 4 - PROGRESS: at 82.19% examples, 374171 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:03,300 : INFO :EPOCH 4 - PROGRESS: at 84.25% examples, 373768 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:04,320 : INFO :EPOCH 4 - PROGRESS: at 86.68% examples, 374621 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:05,326 : INFO :EPOCH 4 - PROGRESS: at 88.95% examples, 374976 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:06,337 : INFO :EPOCH 4 - PROGRESS: at 91.09% examples, 374989 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:07,362 : INFO :EPOCH 4 - PROGRESS: at 93.40% examples, 375350 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:08,369 : INFO :EPOCH 4 - PROGRESS: at 95.45% examples, 375148 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:09,390 : INFO :EPOCH 4 - PROGRESS: at 97.62% examples, 375040 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:10,391 : INFO :EPOCH 4 - PROGRESS: at 99.59% examples, 374491 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 21:32:10,537 : INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 21:32:10,566 : INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 21:32:10,581 : INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 21:32:10,582 : INFO :EPOCH - 4 : training on 23267059 raw words (17561184 effective words) took 46.9s, 374543 effective words/s\n",
      "2019-05-03 21:32:11,600 : INFO :EPOCH 5 - PROGRESS: at 1.84% examples, 324055 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:12,601 : INFO :EPOCH 5 - PROGRESS: at 4.05% examples, 354800 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:13,614 : INFO :EPOCH 5 - PROGRESS: at 6.17% examples, 360896 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:14,632 : INFO :EPOCH 5 - PROGRESS: at 8.38% examples, 363400 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:15,653 : INFO :EPOCH 5 - PROGRESS: at 10.82% examples, 373779 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 21:32:16,655 : INFO :EPOCH 5 - PROGRESS: at 13.13% examples, 379044 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:17,674 : INFO :EPOCH 5 - PROGRESS: at 15.33% examples, 379129 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:18,702 : INFO :EPOCH 5 - PROGRESS: at 17.44% examples, 376056 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:19,769 : INFO :EPOCH 5 - PROGRESS: at 18.37% examples, 350926 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:20,778 : INFO :EPOCH 5 - PROGRESS: at 19.64% examples, 338045 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:21,829 : INFO :EPOCH 5 - PROGRESS: at 21.30% examples, 332064 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:22,890 : INFO :EPOCH 5 - PROGRESS: at 22.76% examples, 324517 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:23,914 : INFO :EPOCH 5 - PROGRESS: at 24.38% examples, 320686 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:24,935 : INFO :EPOCH 5 - PROGRESS: at 25.68% examples, 313806 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:25,937 : INFO :EPOCH 5 - PROGRESS: at 27.21% examples, 310659 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:26,941 : INFO :EPOCH 5 - PROGRESS: at 28.52% examples, 306048 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:27,957 : INFO :EPOCH 5 - PROGRESS: at 29.83% examples, 300992 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:28,979 : INFO :EPOCH 5 - PROGRESS: at 31.00% examples, 295567 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:30,002 : INFO :EPOCH 5 - PROGRESS: at 32.52% examples, 293732 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:31,018 : INFO :EPOCH 5 - PROGRESS: at 33.91% examples, 290723 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:32,029 : INFO :EPOCH 5 - PROGRESS: at 36.23% examples, 296012 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:33,053 : INFO :EPOCH 5 - PROGRESS: at 38.25% examples, 298373 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:34,054 : INFO :EPOCH 5 - PROGRESS: at 40.14% examples, 299573 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:35,072 : INFO :EPOCH 5 - PROGRESS: at 42.30% examples, 302618 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:36,100 : INFO :EPOCH 5 - PROGRESS: at 43.99% examples, 302326 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:37,115 : INFO :EPOCH 5 - PROGRESS: at 45.16% examples, 298298 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:38,157 : INFO :EPOCH 5 - PROGRESS: at 47.15% examples, 299655 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:39,174 : INFO :EPOCH 5 - PROGRESS: at 49.05% examples, 300670 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 21:32:40,211 : INFO :EPOCH 5 - PROGRESS: at 50.74% examples, 300356 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3993c8e4e6b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpermuter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPermuteSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsup_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermuter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, corpus_file, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, docvecs, docvecs_mapfile, comment, trim_rule, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 end_alpha=self.min_alpha, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "permuter = PermuteSentences(unsup_sentences)\n",
    "model = Doc2Vec(permuter, dm=0, hs=1, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7b1674fe6273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# done with training, free up some memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_temporary_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_inference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# done with training, free up some memory\n",
    "model.delete_temporary_training_data(keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
